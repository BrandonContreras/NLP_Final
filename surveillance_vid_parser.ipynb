{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "surveillance_vid_parser.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwLaz3McvrQB"
      },
      "source": [
        "# Intake Video and Produce tensor of needed features for trained Neural Network\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TJTWe5Xv0N4",
        "outputId": "024654fa-1653-4b32-ec5b-b997390059f1"
      },
      "source": [
        "from google.colab import drive\n",
        "import cv2\n",
        "import numpy as np\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZBAoh3exLkC"
      },
      "source": [
        "def video_to_frames(video, out_folder):\n",
        "    \"\"\"Convert mp4 video to still images at 1 per second to give folder name.\"\"\"\n",
        "    # Name each still image after it's timestamp in folder.\n",
        "    # Return list of frames instead\n",
        "    \n",
        "    # Find fps\n",
        "    vidcap = cv2.VideoCapture(video)\n",
        "    fps = int(vidcap.get(cv2.CAP_PROP_FPS))\n",
        "    \n",
        "\n",
        "    # Write image to folder for every 1 second of video\n",
        "    # Name the image after it's frame count\n",
        "    count = 0\n",
        "    read_image = True\n",
        "    num_frames = 0\n",
        "    while read_image:\n",
        "        read_image, img = vidcap.read()\n",
        "        \n",
        "        # Only read one frame per second\n",
        "        if count % fps == 0:\n",
        "            cv2.imwrite(out_folder + '/frame_%d.jpg'%count, img)\n",
        "            num_frames += 1\n",
        "            \n",
        "        count += 1\n",
        "    print(f\"Retrieved {num_frames} frames\")\n",
        "    return fps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNHa8OzRxaWh",
        "outputId": "d73bc198-224f-43ec-85c0-621823e28c99"
      },
      "source": [
        "vid_path = '/content/gdrive/MyDrive/video_data/frames'\n",
        "video_to_frames('/content/gdrive/MyDrive/video_data/vid1.mp4', \"frames\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FPS: 29\n",
            "Retrieved 23 frames\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4WUfAb4HIVq"
      },
      "source": [
        "# DOWNLOADING FROM URL\n",
        "!pip install mhyt --quiet\n",
        "\n",
        "from mhyt import yt_download\n",
        "url = 'https://www.youtube.com/watch?v=LTXzcJNpxNw'\n",
        "yt_download(url, '/content/gdrive/MyDrive/video_data/vid1.mp4')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5qBpB2eJXTp"
      },
      "source": [
        "# Computing features needed for model\n",
        "* Generate CLIP embeddings of all frames. \n",
        "* Store obj detections using DETR that are over 70% "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YLO0vhH6KLH"
      },
      "source": [
        "import math\n",
        "\n",
        "from PIL import Image\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.models import resnet50\n",
        "import torchvision.transforms as T\n",
        "torch.set_grad_enabled(False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi_yDJ0S6ZSw"
      },
      "source": [
        "transform = T.Compose([\n",
        "    T.Resize(800),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "def box_cxcywh_to_xyxy(x):\n",
        "    x_c, y_c, w, h = x.unbind(1)\n",
        "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
        "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
        "    return torch.stack(b, dim=1)\n",
        "\n",
        "def rescale_bboxes(out_bbox, size):\n",
        "    img_w, img_h = size\n",
        "    b = box_cxcywh_to_xyxy(out_bbox)\n",
        "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
        "    return b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NgIHO736i6L"
      },
      "source": [
        "model = torch.hub.load('facebookresearch/detr', 'detr_resnet50', pretrained=True)\n",
        "model.eval();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZY5eKa5HoQe"
      },
      "source": [
        "def get_detections(img, threshold=0.7):\n",
        "  \"\"\" Return a list of detection bounded boxes for each images\"\"\"\n",
        "  # Make this function take in a folder, and produce a dictionary where the image name is the key, and the value is a list of bounding box detections for that image\n",
        "  # This will NOT need to be done for training as our training data will be coco set. \n",
        "\n",
        "  # Normalize input image and run through DETR model\n",
        "  proc_img = transform(img).unsqueeze(0)\n",
        "  outputs = model(proc_img)\n",
        "\n",
        "  # keep only predictions with confidence above threshold level\n",
        "  probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n",
        "  keep = probas.max(-1).values > threshold\n",
        "\n",
        "  bounded_bxs = rescale_bboxes(outputs['pred_boxes'][0, keep], img.size)\n",
        "\n",
        "  detections = []\n",
        "  for prob, (xmin, ymin, xmax, ymax) in zip(probas[keep], bounded_bxs.tolist()):\n",
        "    # Grab index of most probable class.\n",
        "    class_index = prob.argmax()\n",
        "\n",
        "    #class_index = CLASSES[highest_prob]\n",
        "    class_prob = prob[class_index]\n",
        "\n",
        "    detections.append((class_index.item(), class_prob.item(), (xmin, ymin, xmax, ymax)))\n",
        "\n",
        "  return detections"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zQS6eA_5o73"
      },
      "source": [
        "test = get_detections(im, threshold=0.7)\n",
        "\n",
        "for cl_index, cl_prob, (xmin, ymin, xmax, ymax) in test:\n",
        "  print(cl_index)\n",
        "  print(cl_prob)\n",
        "  print((xmin, ymin, xmax, ymax))\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52nAa33P6qRU"
      },
      "source": [
        "# CLIP EMBEDDING\n",
        "# For CLIP model and cosine function\n",
        "!pip install sentence-transformers --quiet\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive')\n",
        "\n",
        "import sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import glob\n",
        "import pickle\n",
        "import zipfile\n",
        "from collections import defaultdict\n",
        "import tqdm.notebook as tq\n",
        "from io import BytesIO"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}