{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_training.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "qqIriy6NUqB-",
        "dkYwyXiMxzJd",
        "rbRUjZfjpDWv",
        "UdI27bhQ1-mj"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixqGgAdRUSd1",
        "outputId": "0d591950-bd95-41c0-9df2-75dfa8cf6217"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0CHf7-iUfRK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a4c8c018-1004-444a-dafe-15c8568eb0e2"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive/coco')\n",
        "sys.path.append('/content/gdrive/MyDrive')\n",
        "\n",
        "import glob\n",
        "import pickle\n",
        "import zipfile\n",
        "import tqdm.notebook as tq\n",
        "\n",
        "import torch\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqIriy6NUqB-"
      },
      "source": [
        "# Fast Loader Class\n",
        "* Normal dataset loading slowed down training too much"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb7QjtWAUujL"
      },
      "source": [
        "class FastTensorDataLoader:\n",
        "    \"\"\"\n",
        "    A DataLoader-like object for a set of tensors that can be much faster than\n",
        "    TensorDataset + DataLoader because dataloader grabs individual indices of\n",
        "    the dataset and calls cat (slow).\n",
        "    Source: https://discuss.pytorch.org/t/dataloader-much-slower-than-manual-batching/27014/6\n",
        "    \"\"\"\n",
        "    def __init__(self, *tensors, batch_size=32, shuffle=False):\n",
        "        \"\"\"\n",
        "        Initialize a FastTensorDataLoader.\n",
        "        :param *tensors: tensors to store. Must have the same length @ dim 0.\n",
        "        :param batch_size: batch size to load.\n",
        "        :param shuffle: if True, shuffle the data *in-place* whenever an\n",
        "            iterator is created out of this object.\n",
        "        :returns: A FastTensorDataLoader.\n",
        "        \"\"\"\n",
        "        assert all(t.shape[0] == tensors[0].shape[0] for t in tensors)\n",
        "        self.tensors = tensors\n",
        "\n",
        "        self.dataset_len = self.tensors[0].shape[0]\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        # Calculate # batches\n",
        "        n_batches, remainder = divmod(self.dataset_len, self.batch_size)\n",
        "        if remainder > 0:\n",
        "            n_batches += 1\n",
        "        self.n_batches = n_batches\n",
        "    def __iter__(self):\n",
        "        if self.shuffle:\n",
        "            r = torch.randperm(self.dataset_len)\n",
        "            self.tensors = [t[r] for t in self.tensors]\n",
        "        self.i = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.i >= self.dataset_len:\n",
        "            raise StopIteration\n",
        "        batch = tuple(t[self.i:self.i+self.batch_size] for t in self.tensors)\n",
        "        self.i += self.batch_size\n",
        "        return batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_batches"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rg_UiIw6U1jO"
      },
      "source": [
        "# Train and Test dataset + dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hh1cr8Ff2up"
      },
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JCVLDBXU8A2"
      },
      "source": [
        "# Loading in train and test tensors. Ram after loading: 7.44gb\n",
        "train_x = torch.load('/content/gdrive/MyDrive/Final_Tensors/train_x.pt').to('cpu')\n",
        "train_y = torch.load('/content/gdrive/MyDrive/Final_Tensors/train_y.pt').to('cpu')\n",
        "\n",
        "eval_x = torch.load('/content/gdrive/MyDrive/Final_Tensors/eval_x.pt').to('cpu')\n",
        "eval_y = torch.load('/content/gdrive/MyDrive/Final_Tensors/eval_y.pt').to('cpu')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VobIfyjuiYrN"
      },
      "source": [
        "test_x = torch.load('/content/gdrive/MyDrive/Final_Tensors/test_x.pt').to('cpu')\n",
        "test_y = torch.load('/content/gdrive/MyDrive/Final_Tensors/test_y.pt').to('cpu')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjGA9fhfgWF3"
      },
      "source": [
        "# Model Parameters\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 256\n",
        "LEARNING_RATE = 0.001\n",
        "INPUT_SIZE = train_x.shape[1]\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss() # no need for last sigmoid node in model\n",
        "\n",
        "# Dataset setup\n",
        "train_batches = FastTensorDataLoader(train_x, train_y, batch_size=BATCH_SIZE, shuffle=True) # serves as dataset and dataloader for tabular data\n",
        "eval_batches = FastTensorDataLoader(eval_x, eval_y, batch_size=BATCH_SIZE, shuffle=True) \n",
        "test_batches = FastTensorDataLoader(test_x, test_y, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfJz-10Nd3TP"
      },
      "source": [
        "def get_accuracy(y_pred, y_true):\n",
        "    y_pred = torch.round(torch.sigmoid(y_pred))\n",
        "\n",
        "    y_pred = y_pred.to('cpu').numpy()\n",
        "    y_true = y_true.to('cpu').numpy()\n",
        "    acc = accuracy_score(y_pred, y_true) * 100\n",
        "\n",
        "    return acc\n",
        "\n",
        "def get_outputs(y_pred):\n",
        "    y_pred = torch.round(torch.sigmoid(y_pred))\n",
        "    y_pred = y_pred.to('cpu').numpy()\n",
        "\n",
        "    ones = len(np.where(y_pred == 1)[0])\n",
        "    zeros = len(np.where(y_pred == 0)[0])\n",
        "\n",
        "    return ones, zeros\n",
        "\n",
        "\n",
        "def validation(model, eval_loader):\n",
        "  model.eval()\n",
        "\n",
        "  y_pred = []\n",
        "  y_actual = [] \n",
        "  \n",
        "  with torch.no_grad():\n",
        "    # Run through each batch of eval dataset\n",
        "    for x_batch, y_batch in eval_loader:\n",
        "      x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "      y_actual_batch = y_batch.unsqueeze(1)\n",
        "      y_pred_batch = model(x_batch.float().to(device))\n",
        "\n",
        "      y_actual.extend(y_actual_batch)\n",
        "      y_pred.extend(y_pred_batch)\n",
        "\n",
        "  return torch.stack(y_pred), torch.stack(y_actual)\n",
        "\n",
        "def binary_acc(y_pred, y_test):\n",
        "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
        "\n",
        "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
        "    acc = correct_results_sum/y_test.shape[0]\n",
        "    acc = torch.round(acc * 100)\n",
        "    \n",
        "    return acc"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dba8_9YauUjN"
      },
      "source": [
        "# Final Model Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3AqBP4oglWR"
      },
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCfOKLHGE5zd"
      },
      "source": [
        "class NLQ_Security(nn.Module): # MODEL 3, same as base just two more hidden layers\n",
        "    def __init__(self):\n",
        "        super(NLQ_Security, self).__init__()\n",
        "\n",
        "        # Input Layer\n",
        "        self.input_layer = nn.Linear(INPUT_SIZE, 64) \n",
        "\n",
        "        # Hidden Layers\n",
        "        self.layer_2 = nn.Linear(64, 64)\n",
        "        self.layer_3 = nn.Linear(64, 64)\n",
        "        self.layer_4 = nn.Linear(64, 64)\n",
        "        \n",
        "        # Out Layer\n",
        "        self.output_layer = nn.Linear(64, 1) \n",
        "        \n",
        "        # Reg features\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
        "        self.batchnorm3 = nn.BatchNorm1d(64)\n",
        "        self.batchnorm4 = nn.BatchNorm1d(64)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        # Relu on input layer outputs, then normalization\n",
        "        x = self.relu(self.input_layer(inputs))\n",
        "        x = self.batchnorm1(x)\n",
        "        \n",
        "        # HL 1\n",
        "        x = self.relu(self.layer_2(x))\n",
        "        x = self.batchnorm2(x)\n",
        "        \n",
        "        x = self.relu(self.layer_3(x))\n",
        "        x = self.batchnorm3(x)\n",
        "        \n",
        "        x = self.relu(self.layer_4(x))\n",
        "        x = self.batchnorm4(x)\n",
        "        \n",
        "        # Out layer\n",
        "        x = self.dropout(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBWLhdkww3hl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df70176b-f3ce-4a4c-f7bd-40940f2859c6"
      },
      "source": [
        "# RUN NEW MODEL AND TRAIN.\n",
        "del model\n",
        "torch.cuda.empty_cache() \n",
        "model = NLQ_Security()\n",
        "model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  model.train()\n",
        "  # Go through each batch in training data\n",
        "  for x_batch, y_batch in train_batches:\n",
        "\n",
        "    # Since training on GPU now\n",
        "    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "    \n",
        "    # Clear out gradients for new batch\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    y_pred = model(x_batch.float())\n",
        "        \n",
        "    loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
        "    acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
        "        \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "        \n",
        "    epoch_loss += loss.item()\n",
        "    epoch_acc += acc.item()\n",
        "\n",
        "  # Check Model's performance on eval set after going through one full epoch of training data\n",
        "  eval_pred, eval_true = validation(model, eval_batches)\n",
        "  eval_acc = get_accuracy(eval_pred, eval_true)\n",
        "  \n",
        "  ones, zeros = get_outputs(eval_pred)\n",
        "  print(f'Ones: {ones}\\nZeros: {zeros}')\n",
        "  print(f'Epoch {epoch+0:03}: | Loss: {epoch_loss/len(train_batches):.5f} | Acc: {epoch_acc/len(train_batches):.3f} | Eval Acc: {eval_acc:.3f}')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ones: 8662\n",
            "Zeros: 32697\n",
            "Epoch 000: | Loss: 0.44530 | Acc: 75.390 | Eval Acc: 78.658\n",
            "Ones: 10223\n",
            "Zeros: 31136\n",
            "Epoch 001: | Loss: 0.34168 | Acc: 82.934 | Eval Acc: 84.555\n",
            "Ones: 9929\n",
            "Zeros: 31430\n",
            "Epoch 002: | Loss: 0.28771 | Acc: 86.271 | Eval Acc: 85.643\n",
            "Ones: 10245\n",
            "Zeros: 31114\n",
            "Epoch 003: | Loss: 0.26469 | Acc: 87.623 | Eval Acc: 86.416\n",
            "Ones: 10605\n",
            "Zeros: 30754\n",
            "Epoch 004: | Loss: 0.24768 | Acc: 88.533 | Eval Acc: 86.499\n",
            "Ones: 10288\n",
            "Zeros: 31071\n",
            "Epoch 005: | Loss: 0.23497 | Acc: 89.246 | Eval Acc: 86.719\n",
            "Ones: 10837\n",
            "Zeros: 30522\n",
            "Epoch 006: | Loss: 0.22378 | Acc: 89.777 | Eval Acc: 86.615\n",
            "Ones: 9933\n",
            "Zeros: 31426\n",
            "Epoch 007: | Loss: 0.21486 | Acc: 90.255 | Eval Acc: 86.760\n",
            "Ones: 11434\n",
            "Zeros: 29925\n",
            "Epoch 008: | Loss: 0.20741 | Acc: 90.666 | Eval Acc: 86.748\n",
            "Ones: 11243\n",
            "Zeros: 30116\n",
            "Epoch 009: | Loss: 0.20023 | Acc: 91.031 | Eval Acc: 86.562\n",
            "Ones: 10646\n",
            "Zeros: 30713\n",
            "Epoch 010: | Loss: 0.19418 | Acc: 91.380 | Eval Acc: 86.932\n",
            "Ones: 10791\n",
            "Zeros: 30568\n",
            "Epoch 011: | Loss: 0.18901 | Acc: 91.589 | Eval Acc: 86.774\n",
            "Ones: 10832\n",
            "Zeros: 30527\n",
            "Epoch 012: | Loss: 0.18346 | Acc: 91.824 | Eval Acc: 86.651\n",
            "Ones: 11479\n",
            "Zeros: 29880\n",
            "Epoch 013: | Loss: 0.17948 | Acc: 92.058 | Eval Acc: 86.586\n",
            "Ones: 10933\n",
            "Zeros: 30426\n",
            "Epoch 014: | Loss: 0.17449 | Acc: 92.309 | Eval Acc: 86.663\n",
            "Ones: 11322\n",
            "Zeros: 30037\n",
            "Epoch 015: | Loss: 0.17034 | Acc: 92.533 | Eval Acc: 86.506\n",
            "Ones: 11315\n",
            "Zeros: 30044\n",
            "Epoch 016: | Loss: 0.16693 | Acc: 92.670 | Eval Acc: 86.649\n",
            "Ones: 11389\n",
            "Zeros: 29970\n",
            "Epoch 017: | Loss: 0.16277 | Acc: 92.858 | Eval Acc: 86.600\n",
            "Ones: 11110\n",
            "Zeros: 30249\n",
            "Epoch 018: | Loss: 0.15969 | Acc: 93.048 | Eval Acc: 86.733\n",
            "Ones: 10686\n",
            "Zeros: 30673\n",
            "Epoch 019: | Loss: 0.15687 | Acc: 93.161 | Eval Acc: 86.545\n",
            "Ones: 10998\n",
            "Zeros: 30361\n",
            "Epoch 020: | Loss: 0.15438 | Acc: 93.270 | Eval Acc: 86.346\n",
            "Ones: 11439\n",
            "Zeros: 29920\n",
            "Epoch 021: | Loss: 0.15054 | Acc: 93.452 | Eval Acc: 86.300\n",
            "Ones: 10354\n",
            "Zeros: 31005\n",
            "Epoch 022: | Loss: 0.14814 | Acc: 93.575 | Eval Acc: 86.569\n",
            "Ones: 11151\n",
            "Zeros: 30208\n",
            "Epoch 023: | Loss: 0.14566 | Acc: 93.747 | Eval Acc: 86.296\n",
            "Ones: 11074\n",
            "Zeros: 30285\n",
            "Epoch 024: | Loss: 0.14271 | Acc: 93.863 | Eval Acc: 86.346\n",
            "Ones: 11398\n",
            "Zeros: 29961\n",
            "Epoch 025: | Loss: 0.14155 | Acc: 93.882 | Eval Acc: 85.931\n",
            "Ones: 11068\n",
            "Zeros: 30291\n",
            "Epoch 026: | Loss: 0.13849 | Acc: 94.060 | Eval Acc: 86.255\n",
            "Ones: 10674\n",
            "Zeros: 30685\n",
            "Epoch 027: | Loss: 0.13656 | Acc: 94.116 | Eval Acc: 86.221\n",
            "Ones: 11073\n",
            "Zeros: 30286\n",
            "Epoch 028: | Loss: 0.13429 | Acc: 94.185 | Eval Acc: 86.068\n",
            "Ones: 11152\n",
            "Zeros: 30207\n",
            "Epoch 029: | Loss: 0.13292 | Acc: 94.291 | Eval Acc: 86.124\n",
            "Ones: 11432\n",
            "Zeros: 29927\n",
            "Epoch 030: | Loss: 0.13060 | Acc: 94.410 | Eval Acc: 86.163\n",
            "Ones: 11190\n",
            "Zeros: 30169\n",
            "Epoch 031: | Loss: 0.12898 | Acc: 94.500 | Eval Acc: 86.071\n",
            "Ones: 11196\n",
            "Zeros: 30163\n",
            "Epoch 032: | Loss: 0.12810 | Acc: 94.510 | Eval Acc: 86.037\n",
            "Ones: 11116\n",
            "Zeros: 30243\n",
            "Epoch 033: | Loss: 0.12546 | Acc: 94.647 | Eval Acc: 85.843\n",
            "Ones: 11537\n",
            "Zeros: 29822\n",
            "Epoch 034: | Loss: 0.12363 | Acc: 94.702 | Eval Acc: 85.827\n",
            "Ones: 10875\n",
            "Zeros: 30484\n",
            "Epoch 035: | Loss: 0.12271 | Acc: 94.806 | Eval Acc: 85.967\n",
            "Ones: 11530\n",
            "Zeros: 29829\n",
            "Epoch 036: | Loss: 0.12167 | Acc: 94.815 | Eval Acc: 85.742\n",
            "Ones: 11075\n",
            "Zeros: 30284\n",
            "Epoch 037: | Loss: 0.12020 | Acc: 94.907 | Eval Acc: 85.967\n",
            "Ones: 11165\n",
            "Zeros: 30194\n",
            "Epoch 038: | Loss: 0.11843 | Acc: 94.962 | Eval Acc: 85.967\n",
            "Ones: 11407\n",
            "Zeros: 29952\n",
            "Epoch 039: | Loss: 0.11718 | Acc: 95.031 | Eval Acc: 85.701\n",
            "Ones: 11586\n",
            "Zeros: 29773\n",
            "Epoch 040: | Loss: 0.11573 | Acc: 95.072 | Eval Acc: 85.607\n",
            "Ones: 10946\n",
            "Zeros: 30413\n",
            "Epoch 041: | Loss: 0.11441 | Acc: 95.129 | Eval Acc: 85.882\n",
            "Ones: 11656\n",
            "Zeros: 29703\n",
            "Epoch 042: | Loss: 0.11282 | Acc: 95.198 | Eval Acc: 85.457\n",
            "Ones: 11126\n",
            "Zeros: 30233\n",
            "Epoch 043: | Loss: 0.11237 | Acc: 95.255 | Eval Acc: 85.969\n",
            "Ones: 11176\n",
            "Zeros: 30183\n",
            "Epoch 044: | Loss: 0.11152 | Acc: 95.257 | Eval Acc: 85.950\n",
            "Ones: 10686\n",
            "Zeros: 30673\n",
            "Epoch 045: | Loss: 0.11016 | Acc: 95.320 | Eval Acc: 85.892\n",
            "Ones: 11465\n",
            "Zeros: 29894\n",
            "Epoch 046: | Loss: 0.10812 | Acc: 95.435 | Eval Acc: 85.585\n",
            "Ones: 11434\n",
            "Zeros: 29925\n",
            "Epoch 047: | Loss: 0.10816 | Acc: 95.403 | Eval Acc: 85.694\n",
            "Ones: 11347\n",
            "Zeros: 30012\n",
            "Epoch 048: | Loss: 0.10705 | Acc: 95.488 | Eval Acc: 85.512\n",
            "Ones: 11275\n",
            "Zeros: 30084\n",
            "Epoch 049: | Loss: 0.10590 | Acc: 95.517 | Eval Acc: 85.556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QArdUtObkOx8"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5o4RlL9GmB-J"
      },
      "source": [
        "def get_predictions(y_pred, y_true):\n",
        "    y_pred = torch.round(torch.sigmoid(y_pred))\n",
        "\n",
        "    y_pred = y_pred.to('cpu').numpy()\n",
        "    y_true = y_true.to('cpu').numpy()\n",
        "\n",
        "    return y_pred, y_true"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8HLHir0lzOJ",
        "outputId": "48827778-2ca5-4c87-e7f1-d316c80022bb"
      },
      "source": [
        "test_pred, test_true = validation(model, test_batches)\n",
        "test_acc = get_accuracy(test_pred, test_true)\n",
        "test_y_hat, test_y_actual = get_predictions(test_pred, test_true)\n",
        "test_acc"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "85.10929964994737"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJ7JgUV8kOCN"
      },
      "source": [
        "cf_matrix= confusion_matrix(test_y_actual, test_y_hat)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "X_c-luManFIA",
        "outputId": "a9e0a8c6-a9e5-425b-cbd2-444be422a436"
      },
      "source": [
        "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
        "\n",
        "\n",
        "group_percentages = ['{0:.2%}'.format(value) for value in\n",
        "                     cf_matrix.flatten()/np.sum(cf_matrix)]\n",
        "                     \n",
        "labels = [f'{v1}\\n{v3}' for v1, v3 in\n",
        "          zip(group_names, group_percentages)]\n",
        "\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fc5de36ee10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVMnO4Svw9k1",
        "outputId": "9c990fad-fa6c-4273-b175-31c88c0b0121"
      },
      "source": [
        "print(classification_report(test_y_actual, test_y_hat))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.90      0.89      0.90     30121\n",
            "         1.0       0.71      0.73      0.72     10730\n",
            "\n",
            "    accuracy                           0.85     40851\n",
            "   macro avg       0.81      0.81      0.81     40851\n",
            "weighted avg       0.85      0.85      0.85     40851\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdI27bhQ1-mj"
      },
      "source": [
        "# Experimental Models with Training Stats logged"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WRbYIQ-yFJU"
      },
      "source": [
        "class Clip_nlp(nn.Module): # MODEL 3, same as base just two more hidden layers\n",
        "    def __init__(self):\n",
        "        super(Clip_nlp, self).__init__()\n",
        "\n",
        "        # Input Layer\n",
        "        self.layer_1 = nn.Linear(INPUT_SIZE, 64) \n",
        "\n",
        "        # Hidden Layers\n",
        "        self.layer_2 = nn.Linear(64, 64)\n",
        "        self.layer_3 = nn.Linear(64, 64)\n",
        "        self.layer_4 = nn.Linear(64, 64)\n",
        "        \n",
        "        # Out Layer\n",
        "        self.layer_out = nn.Linear(64, 1) \n",
        "        \n",
        "        # Reg features\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
        "        self.batchnorm3 = nn.BatchNorm1d(64)\n",
        "        self.batchnorm4 = nn.BatchNorm1d(64)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        # Relu on input layer outputs, then normalization\n",
        "        x = self.relu(self.layer_1(inputs))\n",
        "        x = self.batchnorm1(x)\n",
        "        \n",
        "        # HL 1\n",
        "        x = self.relu(self.layer_2(x))\n",
        "        x = self.batchnorm2(x)\n",
        "        \n",
        "        x = self.relu(self.layer_3(x))\n",
        "        x = self.batchnorm3(x)\n",
        "        \n",
        "        x = self.relu(self.layer_4(x))\n",
        "        x = self.batchnorm4(x)\n",
        "        \n",
        "        # Out layer\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer_out(x)\n",
        "        return x\n",
        "\n",
        "Epoch 000: | Loss: 0.46768 | Acc: 74.186 | Eval Acc: 79.000\n",
        "Epoch 001: | Loss: 0.34383 | Acc: 82.783 | Eval Acc: 84.000\n",
        "Epoch 002: | Loss: 0.29810 | Acc: 85.520 | Eval Acc: 85.000\n",
        "Epoch 003: | Loss: 0.27298 | Acc: 87.072 | Eval Acc: 85.000\n",
        "Epoch 004: | Loss: 0.25464 | Acc: 88.087 | Eval Acc: 86.000\n",
        "Epoch 005: | Loss: 0.24009 | Acc: 88.859 | Eval Acc: 86.000\n",
        "Epoch 006: | Loss: 0.22869 | Acc: 89.460 | Eval Acc: 86.000\n",
        "Epoch 007: | Loss: 0.21797 | Acc: 90.040 | Eval Acc: 86.000\n",
        "Epoch 008: | Loss: 0.20903 | Acc: 90.484 | Eval Acc: 86.000\n",
        "Epoch 009: | Loss: 0.20111 | Acc: 90.951 | Eval Acc: 86.000\n",
        "Epoch 010: | Loss: 0.19349 | Acc: 91.316 | Eval Acc: 86.000\n",
        "Epoch 011: | Loss: 0.18750 | Acc: 91.581 | Eval Acc: 86.000\n",
        "Epoch 012: | Loss: 0.18111 | Acc: 91.939 | Eval Acc: 86.000\n",
        "Epoch 013: | Loss: 0.17548 | Acc: 92.206 | Eval Acc: 86.000\n",
        "Epoch 014: | Loss: 0.17009 | Acc: 92.475 | Eval Acc: 86.000\n",
        "Epoch 015: | Loss: 0.16564 | Acc: 92.740 | Eval Acc: 86.000\n",
        "Epoch 016: | Loss: 0.16137 | Acc: 92.904 | Eval Acc: 86.000\n",
        "Epoch 017: | Loss: 0.15739 | Acc: 93.128 | Eval Acc: 86.000\n",
        "Epoch 018: | Loss: 0.15315 | Acc: 93.300 | Eval Acc: 86.000\n",
        "Epoch 019: | Loss: 0.14974 | Acc: 93.453 | Eval Acc: 86.000\n",
        "Epoch 020: | Loss: 0.14623 | Acc: 93.628 | Eval Acc: 86.000\n",
        "Epoch 021: | Loss: 0.14271 | Acc: 93.794 | Eval Acc: 86.000\n",
        "Epoch 022: | Loss: 0.13975 | Acc: 93.987 | Eval Acc: 85.000\n",
        "Epoch 023: | Loss: 0.13694 | Acc: 94.045 | Eval Acc: 85.000\n",
        "Epoch 024: | Loss: 0.13382 | Acc: 94.191 | Eval Acc: 86.000\n",
        "Epoch 025: | Loss: 0.13061 | Acc: 94.336 | Eval Acc: 85.000\n",
        "Epoch 026: | Loss: 0.12873 | Acc: 94.424 | Eval Acc: 85.000\n",
        "Epoch 027: | Loss: 0.12690 | Acc: 94.534 | Eval Acc: 85.000\n",
        "Epoch 028: | Loss: 0.12491 | Acc: 94.623 | Eval Acc: 85.000\n",
        "Epoch 029: | Loss: 0.12201 | Acc: 94.771 | Eval Acc: 85.000\n",
        "Epoch 030: | Loss: 0.12000 | Acc: 94.821 | Eval Acc: 85.000\n",
        "Epoch 031: | Loss: 0.11785 | Acc: 94.935 | Eval Acc: 85.000\n",
        "Epoch 032: | Loss: 0.11654 | Acc: 95.025 | Eval Acc: 85.000\n",
        "Epoch 033: | Loss: 0.11376 | Acc: 95.179 | Eval Acc: 85.000\n",
        "Epoch 034: | Loss: 0.11154 | Acc: 95.280 | Eval Acc: 85.000\n",
        "Epoch 035: | Loss: 0.11076 | Acc: 95.303 | Eval Acc: 85.000\n",
        "Epoch 036: | Loss: 0.10847 | Acc: 95.410 | Eval Acc: 85.000\n",
        "Epoch 037: | Loss: 0.10660 | Acc: 95.455 | Eval Acc: 85.000\n",
        "Epoch 038: | Loss: 0.10718 | Acc: 95.493 | Eval Acc: 85.000\n",
        "Epoch 039: | Loss: 0.10453 | Acc: 95.554 | Eval Acc: 85.000\n",
        "Epoch 040: | Loss: 0.10231 | Acc: 95.641 | Eval Acc: 85.000\n",
        "Epoch 041: | Loss: 0.10184 | Acc: 95.686 | Eval Acc: 85.000\n",
        "Epoch 042: | Loss: 0.09975 | Acc: 95.774 | Eval Acc: 85.000\n",
        "Epoch 043: | Loss: 0.09869 | Acc: 95.852 | Eval Acc: 85.000\n",
        "Epoch 044: | Loss: 0.09733 | Acc: 95.859 | Eval Acc: 85.000\n",
        "Epoch 045: | Loss: 0.09572 | Acc: 95.922 | Eval Acc: 85.000\n",
        "Epoch 046: | Loss: 0.09523 | Acc: 95.944 | Eval Acc: 85.000\n",
        "Epoch 047: | Loss: 0.09390 | Acc: 96.020 | Eval Acc: 84.000\n",
        "Epoch 048: | Loss: 0.09205 | Acc: 96.103 | Eval Acc: 85.000\n",
        "Epoch 049: | Loss: 0.09159 | Acc: 96.114 | Eval Acc: 85.000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3q-rAmO2Rkf"
      },
      "source": [
        "# MODEL 3: 128 at 3 hidden layers\n",
        "class Clip_nlp(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Clip_nlp, self).__init__()\n",
        "\n",
        "        # Input Layer\n",
        "        self.layer_1 = nn.Linear(INPUT_SIZE, 128) \n",
        "\n",
        "        # Hidden Layers\n",
        "        self.layer_2 = nn.Linear(128, 128)\n",
        "        self.layer_3 = nn.Linear(128, 128)\n",
        "        self.layer_4 = nn.Linear(128, 128)\n",
        "        \n",
        "        # Out Layer\n",
        "        self.layer_out = nn.Linear(128, 1) \n",
        "        \n",
        "        # Reg features\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.batchnorm1 = nn.BatchNorm1d(128)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(128)\n",
        "        self.batchnorm3 = nn.BatchNorm1d(128)\n",
        "        self.batchnorm4 = nn.BatchNorm1d(128)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        # Relu on input layer outputs, then normalization\n",
        "        x = self.relu(self.layer_1(inputs))\n",
        "        x = self.batchnorm1(x)\n",
        "        \n",
        "        # HL 1\n",
        "        x = self.relu(self.layer_2(x))\n",
        "        x = self.batchnorm2(x)\n",
        "        \n",
        "        x = self.relu(self.layer_3(x))\n",
        "        x = self.batchnorm3(x)\n",
        "        \n",
        "        x = self.relu(self.layer_4(x))\n",
        "        x = self.batchnorm4(x)\n",
        "        \n",
        "        # Out layer\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer_out(x)\n",
        "        return x\n",
        "\n",
        "Epoch 000: | Loss: 0.46243 | Acc: 74.522 | Eval Acc: 79.000\n",
        "Epoch 001: | Loss: 0.33179 | Acc: 83.482 | Eval Acc: 84.000\n",
        "Epoch 002: | Loss: 0.28039 | Acc: 86.540 | Eval Acc: 85.000\n",
        "Epoch 003: | Loss: 0.25104 | Acc: 88.283 | Eval Acc: 86.000\n",
        "Epoch 004: | Loss: 0.22831 | Acc: 89.453 | Eval Acc: 86.000\n",
        "Epoch 005: | Loss: 0.21008 | Acc: 90.462 | Eval Acc: 86.000\n",
        "Epoch 006: | Loss: 0.19428 | Acc: 91.280 | Eval Acc: 86.000\n",
        "Epoch 007: | Loss: 0.18085 | Acc: 91.939 | Eval Acc: 86.000\n",
        "Epoch 008: | Loss: 0.16815 | Acc: 92.587 | Eval Acc: 86.000\n",
        "Epoch 009: | Loss: 0.15665 | Acc: 93.112 | Eval Acc: 86.000\n",
        "Epoch 010: | Loss: 0.14697 | Acc: 93.585 | Eval Acc: 86.000\n",
        "Epoch 011: | Loss: 0.13709 | Acc: 94.052 | Eval Acc: 86.000\n",
        "Epoch 012: | Loss: 0.12824 | Acc: 94.527 | Eval Acc: 86.000\n",
        "Epoch 013: | Loss: 0.12176 | Acc: 94.854 | Eval Acc: 86.000\n",
        "Epoch 014: | Loss: 0.11399 | Acc: 95.184 | Eval Acc: 86.000\n",
        "Epoch 015: | Loss: 0.10805 | Acc: 95.433 | Eval Acc: 86.000\n",
        "Epoch 016: | Loss: 0.10220 | Acc: 95.749 | Eval Acc: 86.000\n",
        "Epoch 017: | Loss: 0.09671 | Acc: 95.908 | Eval Acc: 85.000\n",
        "Epoch 018: | Loss: 0.09137 | Acc: 96.179 | Eval Acc: 86.000\n",
        "Epoch 019: | Loss: 0.08704 | Acc: 96.361 | Eval Acc: 86.000\n",
        "Epoch 020: | Loss: 0.08304 | Acc: 96.545 | Eval Acc: 86.000\n",
        "Epoch 021: | Loss: 0.07889 | Acc: 96.726 | Eval Acc: 86.000\n",
        "Epoch 022: | Loss: 0.07587 | Acc: 96.850 | Eval Acc: 86.000\n",
        "Epoch 023: | Loss: 0.07294 | Acc: 96.960 | Eval Acc: 85.000\n",
        "Epoch 024: | Loss: 0.07009 | Acc: 97.126 | Eval Acc: 86.000\n",
        "Epoch 025: | Loss: 0.06714 | Acc: 97.280 | Eval Acc: 85.000\n",
        "Epoch 026: | Loss: 0.06463 | Acc: 97.381 | Eval Acc: 86.000\n",
        "Epoch 027: | Loss: 0.06148 | Acc: 97.507 | Eval Acc: 86.000\n",
        "Epoch 028: | Loss: 0.06007 | Acc: 97.585 | Eval Acc: 85.000\n",
        "Epoch 029: | Loss: 0.05807 | Acc: 97.648 | Eval Acc: 85.000\n",
        "Epoch 030: | Loss: 0.05525 | Acc: 97.767 | Eval Acc: 86.000\n",
        "Epoch 031: | Loss: 0.05463 | Acc: 97.836 | Eval Acc: 85.000\n",
        "Epoch 032: | Loss: 0.05228 | Acc: 97.922 | Eval Acc: 85.000\n",
        "Epoch 033: | Loss: 0.04993 | Acc: 98.025 | Eval Acc: 86.000\n",
        "Epoch 034: | Loss: 0.05048 | Acc: 98.007 | Eval Acc: 85.000\n",
        "Epoch 035: | Loss: 0.04758 | Acc: 98.126 | Eval Acc: 85.000\n",
        "Epoch 036: | Loss: 0.04580 | Acc: 98.229 | Eval Acc: 85.000\n",
        "Epoch 037: | Loss: 0.04513 | Acc: 98.217 | Eval Acc: 85.000\n",
        "Epoch 038: | Loss: 0.04368 | Acc: 98.298 | Eval Acc: 85.000\n",
        "Epoch 039: | Loss: 0.04244 | Acc: 98.352 | Eval Acc: 85.000\n",
        "Epoch 040: | Loss: 0.04151 | Acc: 98.406 | Eval Acc: 85.000\n",
        "Epoch 041: | Loss: 0.04181 | Acc: 98.399 | Eval Acc: 85.000\n",
        "Epoch 042: | Loss: 0.03917 | Acc: 98.491 | Eval Acc: 86.000\n",
        "Epoch 043: | Loss: 0.03946 | Acc: 98.464 | Eval Acc: 85.000\n",
        "Epoch 044: | Loss: 0.03796 | Acc: 98.552 | Eval Acc: 85.000\n",
        "Epoch 045: | Loss: 0.03555 | Acc: 98.661 | Eval Acc: 85.000\n",
        "Epoch 046: | Loss: 0.03697 | Acc: 98.608 | Eval Acc: 85.000\n",
        "Epoch 047: | Loss: 0.03471 | Acc: 98.688 | Eval Acc: 85.000\n",
        "Epoch 048: | Loss: 0.03548 | Acc: 98.673 | Eval Acc: 85.000\n",
        "Epoch 049: | Loss: 0.03338 | Acc: 98.769 | Eval Acc: 85.000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Vzb-Z2jJTdx"
      },
      "source": [
        "# Model 4, 3 hidden layers with leakyrelu \n",
        "class Clip_nlp(nn.Module): # MODEL 3\n",
        "    def __init__(self):\n",
        "        super(Clip_nlp, self).__init__()\n",
        "\n",
        "        # Input Layer\n",
        "        self.layer_1 = nn.Linear(INPUT_SIZE, 64) \n",
        "\n",
        "        # Hidden Layers\n",
        "        self.layer_2 = nn.Linear(64, 64)\n",
        "        self.layer_3 = nn.Linear(64, 64)\n",
        "        self.layer_4 = nn.Linear(64, 64)\n",
        "        \n",
        "        # Out Layer\n",
        "        self.layer_out = nn.Linear(64, 1) \n",
        "        \n",
        "        # Reg features\n",
        "        self.relu = nn.LeakyReLU()\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
        "        self.batchnorm3 = nn.BatchNorm1d(64)\n",
        "        self.batchnorm4 = nn.BatchNorm1d(64)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        # Relu on input layer outputs, then normalization\n",
        "        x = self.relu(self.layer_1(inputs))\n",
        "        x = self.batchnorm1(x)\n",
        "        \n",
        "        # HL 1\n",
        "        x = self.relu(self.layer_2(x))\n",
        "        x = self.batchnorm2(x)\n",
        "        \n",
        "        x = self.relu(self.layer_3(x))\n",
        "        x = self.batchnorm3(x)\n",
        "        \n",
        "        x = self.relu(self.layer_4(x))\n",
        "        x = self.batchnorm4(x)\n",
        "        \n",
        "        # Out layer\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer_out(x)\n",
        "        return x\n",
        "        \n",
        "Epoch 000: | Loss: 0.47582 | Acc: 73.446 | Eval Acc: 78.000\n",
        "Epoch 001: | Loss: 0.36522 | Acc: 81.155 | Eval Acc: 83.000\n",
        "Epoch 002: | Loss: 0.30022 | Acc: 85.397 | Eval Acc: 84.000\n",
        "Epoch 003: | Loss: 0.27188 | Acc: 87.126 | Eval Acc: 86.000\n",
        "Epoch 004: | Loss: 0.25211 | Acc: 88.240 | Eval Acc: 86.000\n",
        "Epoch 005: | Loss: 0.23767 | Acc: 88.987 | Eval Acc: 85.000\n",
        "Epoch 006: | Loss: 0.22469 | Acc: 89.709 | Eval Acc: 86.000\n",
        "Epoch 007: | Loss: 0.21363 | Acc: 90.285 | Eval Acc: 86.000\n",
        "Epoch 008: | Loss: 0.20492 | Acc: 90.767 | Eval Acc: 86.000\n",
        "Epoch 009: | Loss: 0.19612 | Acc: 91.204 | Eval Acc: 86.000\n",
        "Epoch 010: | Loss: 0.18808 | Acc: 91.610 | Eval Acc: 86.000\n",
        "Epoch 011: | Loss: 0.18179 | Acc: 91.879 | Eval Acc: 86.000\n",
        "Epoch 012: | Loss: 0.17500 | Acc: 92.314 | Eval Acc: 86.000\n",
        "Epoch 013: | Loss: 0.16994 | Acc: 92.507 | Eval Acc: 86.000\n",
        "Epoch 014: | Loss: 0.16363 | Acc: 92.812 | Eval Acc: 86.000\n",
        "Epoch 015: | Loss: 0.15873 | Acc: 93.056 | Eval Acc: 85.000\n",
        "Epoch 016: | Loss: 0.15450 | Acc: 93.251 | Eval Acc: 86.000\n",
        "Epoch 017: | Loss: 0.14998 | Acc: 93.446 | Eval Acc: 86.000\n",
        "Epoch 018: | Loss: 0.14567 | Acc: 93.704 | Eval Acc: 86.000\n",
        "Epoch 019: | Loss: 0.14198 | Acc: 93.870 | Eval Acc: 86.000\n",
        "Epoch 020: | Loss: 0.13837 | Acc: 94.036 | Eval Acc: 85.000\n",
        "Epoch 021: | Loss: 0.13466 | Acc: 94.229 | Eval Acc: 86.000\n",
        "Epoch 022: | Loss: 0.13112 | Acc: 94.348 | Eval Acc: 85.000\n",
        "Epoch 023: | Loss: 0.12788 | Acc: 94.543 | Eval Acc: 85.000\n",
        "Epoch 024: | Loss: 0.12453 | Acc: 94.717 | Eval Acc: 85.000\n",
        "Epoch 025: | Loss: 0.12187 | Acc: 94.821 | Eval Acc: 85.000\n",
        "Epoch 026: | Loss: 0.11956 | Acc: 94.922 | Eval Acc: 85.000\n",
        "Epoch 027: | Loss: 0.11734 | Acc: 95.047 | Eval Acc: 85.000\n",
        "Epoch 028: | Loss: 0.11406 | Acc: 95.197 | Eval Acc: 85.000\n",
        "Epoch 029: | Loss: 0.11156 | Acc: 95.316 | Eval Acc: 85.000\n",
        "Epoch 030: | Loss: 0.10962 | Acc: 95.401 | Eval Acc: 85.000\n",
        "Epoch 031: | Loss: 0.10743 | Acc: 95.500 | Eval Acc: 85.000\n",
        "Epoch 032: | Loss: 0.10545 | Acc: 95.581 | Eval Acc: 85.000\n",
        "Epoch 033: | Loss: 0.10307 | Acc: 95.711 | Eval Acc: 85.000\n",
        "Epoch 034: | Loss: 0.10205 | Acc: 95.711 | Eval Acc: 85.000\n",
        "Epoch 035: | Loss: 0.09929 | Acc: 95.870 | Eval Acc: 85.000\n",
        "Epoch 036: | Loss: 0.09718 | Acc: 95.928 | Eval Acc: 85.000\n",
        "Epoch 037: | Loss: 0.09609 | Acc: 95.973 | Eval Acc: 85.000\n",
        "Epoch 038: | Loss: 0.09295 | Acc: 96.087 | Eval Acc: 85.000\n",
        "Epoch 039: | Loss: 0.09235 | Acc: 96.157 | Eval Acc: 85.000\n",
        "Epoch 040: | Loss: 0.09075 | Acc: 96.215 | Eval Acc: 85.000\n",
        "Epoch 041: | Loss: 0.08895 | Acc: 96.305 | Eval Acc: 85.000\n",
        "Epoch 042: | Loss: 0.08793 | Acc: 96.370 | Eval Acc: 85.000\n",
        "Epoch 043: | Loss: 0.08682 | Acc: 96.374 | Eval Acc: 85.000\n",
        "Epoch 044: | Loss: 0.08563 | Acc: 96.487 | Eval Acc: 85.000\n",
        "Epoch 045: | Loss: 0.08374 | Acc: 96.498 | Eval Acc: 85.000\n",
        "Epoch 046: | Loss: 0.08298 | Acc: 96.619 | Eval Acc: 85.000\n",
        "Epoch 047: | Loss: 0.08138 | Acc: 96.641 | Eval Acc: 85.000\n",
        "Epoch 048: | Loss: 0.08055 | Acc: 96.688 | Eval Acc: 85.000\n",
        "Epoch 049: | Loss: 0.07852 | Acc: 96.776 | Eval Acc: 85.000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QglIcY1eNOs7"
      },
      "source": [
        "# Fat model 1\n",
        "class Clip_nlp(nn.Module): # MODEL 3\n",
        "    def __init__(self):\n",
        "        super(Clip_nlp, self).__init__()\n",
        "\n",
        "        # Input Layer\n",
        "        self.layer_1 = nn.Linear(INPUT_SIZE, 512) \n",
        "\n",
        "        # Hidden Layers\n",
        "        self.layer_2 = nn.Linear(512, 512)\n",
        "        self.layer_3 = nn.Linear(512, 512)\n",
        "\n",
        "        self.layer_4 = nn.Linear(512, 128)\n",
        "\n",
        "        self.layer_5 = nn.Linear(128, 64)\n",
        "        self.layer_6 = nn.Linear(64, 64)\n",
        "        \n",
        "        # Out Layer\n",
        "        self.layer_out = nn.Linear(64, 1) \n",
        "        \n",
        "        # Reg features\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.batchnorm1 = nn.BatchNorm1d(512)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(512)\n",
        "        self.batchnorm3 = nn.BatchNorm1d(512)\n",
        "        self.batchnorm4 = nn.BatchNorm1d(128)\n",
        "        self.batchnorm5 = nn.BatchNorm1d(64)\n",
        "        self.batchnorm6 = nn.BatchNorm1d(64)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        # Relu on input layer outputs, then normalization\n",
        "        x = self.relu(self.layer_1(inputs))\n",
        "        x = self.batchnorm1(x)\n",
        "        \n",
        "        # HL 1\n",
        "        x = self.relu(self.layer_2(x))\n",
        "        x = self.batchnorm2(x)\n",
        "        # HL 2\n",
        "        x = self.relu(self.layer_3(x))\n",
        "        x = self.batchnorm3(x)\n",
        "        # HL 3\n",
        "        x = self.relu(self.layer_4(x))\n",
        "        x = self.batchnorm4(x)\n",
        "        # HL 4\n",
        "        x = self.relu(self.layer_5(x))\n",
        "        x = self.batchnorm5(x)\n",
        "        # HL 5\n",
        "        x = self.relu(self.layer_6(x))\n",
        "        x = self.batchnorm6(x)    \n",
        "\n",
        "        # Out layer\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer_out(x)\n",
        "        return x\n",
        "\n",
        "Epoch 000: | Loss: 0.46227 | Acc: 74.480 | Eval Acc: 78.000\n",
        "Epoch 001: | Loss: 0.33834 | Acc: 83.036 | Eval Acc: 85.000\n",
        "Epoch 002: | Loss: 0.27191 | Acc: 87.038 | Eval Acc: 86.000\n",
        "Epoch 003: | Loss: 0.23435 | Acc: 89.186 | Eval Acc: 86.000\n",
        "Epoch 004: | Loss: 0.20497 | Acc: 90.767 | Eval Acc: 87.000\n",
        "Epoch 005: | Loss: 0.18020 | Acc: 91.964 | Eval Acc: 86.000\n",
        "Epoch 006: | Loss: 0.15955 | Acc: 92.984 | Eval Acc: 87.000\n",
        "Epoch 007: | Loss: 0.14030 | Acc: 93.960 | Eval Acc: 87.000\n",
        "Epoch 008: | Loss: 0.12399 | Acc: 94.720 | Eval Acc: 86.000\n",
        "Epoch 009: | Loss: 0.10927 | Acc: 95.390 | Eval Acc: 86.000\n",
        "Epoch 010: | Loss: 0.09530 | Acc: 96.018 | Eval Acc: 86.000\n",
        "Epoch 011: | Loss: 0.08552 | Acc: 96.426 | Eval Acc: 86.000\n",
        "Epoch 012: | Loss: 0.07547 | Acc: 96.897 | Eval Acc: 86.000\n",
        "Epoch 013: | Loss: 0.06741 | Acc: 97.244 | Eval Acc: 86.000\n",
        "Epoch 014: | Loss: 0.06086 | Acc: 97.547 | Eval Acc: 86.000\n",
        "Epoch 015: | Loss: 0.05580 | Acc: 97.776 | Eval Acc: 86.000\n",
        "Epoch 016: | Loss: 0.05125 | Acc: 97.942 | Eval Acc: 86.000\n",
        "Epoch 017: | Loss: 0.04683 | Acc: 98.139 | Eval Acc: 86.000\n",
        "Epoch 018: | Loss: 0.04367 | Acc: 98.276 | Eval Acc: 86.000\n",
        "Epoch 019: | Loss: 0.04096 | Acc: 98.413 | Eval Acc: 86.000\n",
        "Epoch 020: | Loss: 0.03806 | Acc: 98.554 | Eval Acc: 86.000\n",
        "Epoch 021: | Loss: 0.03584 | Acc: 98.628 | Eval Acc: 86.000\n",
        "Epoch 022: | Loss: 0.03330 | Acc: 98.744 | Eval Acc: 86.000\n",
        "Epoch 023: | Loss: 0.03220 | Acc: 98.787 | Eval Acc: 86.000\n",
        "Epoch 024: | Loss: 0.03082 | Acc: 98.852 | Eval Acc: 86.000\n",
        "Epoch 025: | Loss: 0.02930 | Acc: 98.910 | Eval Acc: 86.000\n",
        "Epoch 026: | Loss: 0.02819 | Acc: 98.978 | Eval Acc: 86.000\n",
        "Epoch 027: | Loss: 0.02683 | Acc: 98.975 | Eval Acc: 86.000\n",
        "Epoch 028: | Loss: 0.02539 | Acc: 99.029 | Eval Acc: 86.000\n",
        "Epoch 029: | Loss: 0.02479 | Acc: 99.043 | Eval Acc: 86.000\n",
        "Epoch 030: | Loss: 0.02384 | Acc: 99.085 | Eval Acc: 86.000\n",
        "Epoch 031: | Loss: 0.02342 | Acc: 99.128 | Eval Acc: 86.000\n",
        "Epoch 032: | Loss: 0.02211 | Acc: 99.168 | Eval Acc: 86.000\n",
        "Epoch 033: | Loss: 0.02127 | Acc: 99.164 | Eval Acc: 86.000\n",
        "Epoch 034: | Loss: 0.02096 | Acc: 99.184 | Eval Acc: 86.000\n",
        "Epoch 035: | Loss: 0.02030 | Acc: 99.229 | Eval Acc: 86.000\n",
        "Epoch 036: | Loss: 0.01950 | Acc: 99.249 | Eval Acc: 86.000\n",
        "Epoch 037: | Loss: 0.01863 | Acc: 99.307 | Eval Acc: 86.000\n",
        "Epoch 038: | Loss: 0.01888 | Acc: 99.278 | Eval Acc: 86.000\n",
        "Epoch 039: | Loss: 0.01841 | Acc: 99.289 | Eval Acc: 86.000\n",
        "Epoch 040: | Loss: 0.01713 | Acc: 99.408 | Eval Acc: 86.000\n",
        "Epoch 041: | Loss: 0.01741 | Acc: 99.379 | Eval Acc: 86.000\n",
        "Epoch 042: | Loss: 0.01725 | Acc: 99.379 | Eval Acc: 86.000\n",
        "Epoch 043: | Loss: 0.01573 | Acc: 99.491 | Eval Acc: 86.000\n",
        "Epoch 044: | Loss: 0.01678 | Acc: 99.383 | Eval Acc: 86.000\n",
        "Epoch 045: | Loss: 0.01525 | Acc: 99.513 | Eval Acc: 86.000\n",
        "Epoch 046: | Loss: 0.01520 | Acc: 99.502 | Eval Acc: 86.000\n",
        "Epoch 047: | Loss: 0.01521 | Acc: 99.525 | Eval Acc: 86.000\n",
        "Epoch 048: | Loss: 0.01470 | Acc: 99.540 | Eval Acc: 86.000\n",
        "Epoch 049: | Loss: 0.01382 | Acc: 99.561 | Eval Acc: 86.000"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}